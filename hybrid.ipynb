{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!nvidia-smi\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive')\n",
    "os.getcwd()\n",
    "os.chdir('/content/drive/My Drive/QML/demo')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# !pip install pennylane==0.23.0\n",
    "\n",
    "# !pip install autoray==0.2.5\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from math import ceil\n",
    "from math import pi\n",
    "\n",
    "\n",
    "filtered_classes = ['bear','tiger']\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_qubits = 4\n",
    "n_layers = 1\n",
    "n_class = 2\n",
    "n_features = 196\n",
    "image_x_y_dim = 14\n",
    "kernel_size = n_qubits\n",
    "stride = 2\n",
    "kernel_size=2\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "def circuit(inputs, weights):\n",
    "    var_per_qubit = int(len(inputs) / n_qubits) + 1\n",
    "    encoding_gates = ['RZ', 'RY'] * ceil(var_per_qubit / 2)\n",
    "\n",
    "    for qub in range(n_qubits):\n",
    "        qml.Hadamard(wires=qub)\n",
    "        for i in range(var_per_qubit):\n",
    "            idx = qub * var_per_qubit + i\n",
    "            if idx < len(inputs):\n",
    "                if encoding_gates[i] == 'RZ':\n",
    "                    qml.RZ(inputs[idx], wires=qub)\n",
    "                elif encoding_gates[i] == 'RY':\n",
    "                    qml.RY(inputs[idx], wires=qub)\n",
    "\n",
    "    for l in range(n_layers):\n",
    "        for i in range(n_qubits):\n",
    "            qml.CRZ(weights[l, i], wires=[i, (i + 1) % n_qubits])\n",
    "        for j in range(n_qubits, 2 * n_qubits):\n",
    "            qml.RY(weights[l, j], wires=j % n_qubits)\n",
    "\n",
    "    _expectations = [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "    return _expectations\n",
    "\n",
    "class Quanv2d(nn.Module):\n",
    "    def __init__(self, kernel_size=None, stride=None):\n",
    "        super(Quanv2d, self).__init__()\n",
    "        weight_shapes = {\"weights\": (n_layers, 2 * n_qubits)}\n",
    "        qnode = qml.QNode(circuit, dev, interface='torch', diff_method='best')\n",
    "        self.ql1 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        assert len(X.shape) == 4\n",
    "        bs = X.shape[0]\n",
    "        XL = []\n",
    "        for i in range(0, X.shape[2] - 2, stride):\n",
    "            for j in range(0, X.shape[3] - 2, stride):\n",
    "                XL.append(self.ql1(torch.flatten(X[:, :, i:i + kernel_size, j:j + kernel_size], start_dim=1)))\n",
    "        X = torch.cat(XL, dim=1).view(bs,4,6,6)\n",
    "        return X\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # define nn\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.ql1 = Quanv2d(kernel_size=kernel_size, stride=stride)\n",
    "        self.conv1 = nn.Conv2d(4,16,3,stride=1)\n",
    "        self.fc1 = nn.Linear(16*4*4, n_class * 2)\n",
    "        self.lr1 = nn.LeakyReLU(0.1)\n",
    "        self.fc2 = nn.Linear(n_class * 2, n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        bs = X.shape[0]\n",
    "        X = X.view(bs, 1, image_x_y_dim, image_x_y_dim)\n",
    "        X = self.ql1(X) #output(bs, 4, 6, 6)\n",
    "        X = self.lr1(self.conv1(X))\n",
    "        X = X.view(bs,-1)\n",
    "        X = self.fc1(X)\n",
    "        X = self.lr1(X)\n",
    "        X = self.fc2(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "def demo(model):\n",
    "\n",
    "    with open('demo12', 'rb') as fo:\n",
    "        demodict = pickle.load(fo, encoding='bytes')\n",
    "    torch.no_grad()\n",
    "    fig,ax = plt.subplots(3, 4,figsize=(14,9))\n",
    "    t1=time.time()\n",
    "    correct=0\n",
    "    correct_labels=[0]*6+[1]*6\n",
    "    for i, origdata in enumerate(demodict[b'data']):\n",
    "        origdata=origdata.reshape((3,32*32)).transpose()\n",
    "        origdata=origdata.reshape(32,32,3)\n",
    "\n",
    "######### BELOW:preprocess the origdata for model input and predict using the model #######\n",
    "######### modify code in this block to predict ######################################\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),  \n",
    "            transforms.Grayscale(num_output_channels=1), \n",
    "            transforms.Resize((14, 14)),  \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.485,), (0.229,))  \n",
    "])\n",
    "        data=transform(origdata)\n",
    "        data=data.reshape((1,1,14,14))\n",
    "        output=model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        print(type(pred),pred.item(),correct_labels[i])\n",
    "        if pred.item()==correct_labels[i]:correct+=1\n",
    "###############################################################################################\n",
    "###########################  Do not change the code below #####################################\n",
    "        ax[i//4][i%4].axis('off')\n",
    "        ax[i//4][i%4].set_title(f'predicted: {filtered_classes[pred]}')\n",
    "        ax[i//4][i%4].imshow(origdata)\n",
    "    t2=time.time()\n",
    "    fig.suptitle('time taken={:6f} sec. Correct images {}'.format(t2-t1,correct),fontsize=16)\n",
    "    plt.savefig('ex.png')\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "if __name__ == '__main__':\n",
    "    ######### BELOW: load your model ###########\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load('qmodel E2.pth'))\n",
    "    model.eval()\n",
    "    ####################################################\n",
    "    demo(model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
